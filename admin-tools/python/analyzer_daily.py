
import sqlite3
import os
import json
import logging
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import requests

# Logging Setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Config
DB_PATH = os.path.join(os.path.dirname(__file__), '../../dailyport.db')
TODAY = datetime.now().strftime("%Y-%m-%d") # Supabase ISO Format
TODAY_DB = datetime.now().strftime("%Y%m%d") # SQLite Format

# Load Env
env_path = os.path.join(os.path.dirname(__file__), '.env')
if not os.path.exists(env_path):
    env_path = os.path.join(os.path.dirname(__file__), '../.env.local')
if not os.path.exists(env_path):
    env_path = os.path.join(os.path.dirname(__file__), '../../.env.local')
load_dotenv(env_path)

SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    logger.error("Missing Supabase credentials.")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row

# --- Algo Picks Refinement (v5) Infrastructure ---
# Strategy Metadata & Groups
STRATEGY_META = {
    "Value_Picks": {
        "id": "Value_Picks",
        "group": "Fundamental",
        "tags": ["ì €í‰ê°€", "ìš°ëŸ‰ì£¼"],
        "description": "ì €í‰ê°€ ìš°ëŸ‰ì£¼: ROE 10%â†‘, ì˜ì—…ì´ìµë¥  5%â†‘ ê³ ë°°ë‹¹/ì €PBR ê¸°ì—…",
        "mcap_override": None,
        "version": "v5.0"
    },
    "Twin_Engines": {
        "id": "Twin_Engines",
        "group": "Flow",
        "tags": ["ìˆ˜ê¸‰ê°•ë„", "ë™ë°˜ë§¤ìˆ˜"],
        "description": "ìŒëŒì´ ë§¤ìˆ˜: ì™¸ì¸/ê¸°ê´€ í•©ì‚° ìˆ˜ê¸‰ê°•ë„(0.05%â†‘) ë° 2/3ì¼ ì—°ì† ë§¤ìˆ˜",
        "mcap_override": 300000000000, # 3,000ì–µ
        "version": "v5.0"
    },
    "Foreigner_Accumulation": {
        "id": "Foreigner_Accumulation",
        "group": "Flow",
        "tags": ["ì™¸ì¸ë§¤ì§‘", "ë°•ìŠ¤ê¶Œ"],
        "description": "ì™¸ì¸ ë§¤ì§‘: ë°•ìŠ¤ê¶Œ(ë³€ë™í­ 10%â†“) ë‚´ ì™¸ì¸ ë¹„ì¤‘ ì¦ê°€ ë° ì´í‰ì„  ì§€ì§€",
        "mcap_override": None,
        "version": "v5.0"
    },
    "Trend_Following": {
        "id": "Trend_Following",
        "group": "Price",
        "tags": ["ì¶”ì„¸ì¶”ì¢…", "ëŒíŒŒ"],
        "description": "ì¶”ì„¸ì¶”ì¢…: ê±°ë˜í­ë°œ(1.5xâ†‘) ëŒíŒŒ ì¢…ëª© (RSI ê³¼ì—´ ë° ìœ—ê¼¬ë¦¬ ì €í•­ í•„í„°ë§)",
        "mcap_override": None,
        "version": "v5.0"
    }
}

GROUP_WEIGHT = {
    "Flow": 1.0,
    "Price": 1.0,
    "Fundamental": 1.0
}

# Targeted Debugging
DEBUG_TICKERS = [] # Add tickers like "005930" to debug filtration
IS_PROD = os.getenv("NODE_ENV") == "production"

def validate_meta():
    """Verify all strategies in meta have a corresponding implementation logic later."""
    required_strategies = ["Value_Picks", "Twin_Engines", "Foreigner_Accumulation", "Trend_Following"]
    for s_id in required_strategies:
        if s_id not in STRATEGY_META:
            msg = f"CRITICAL: Strategy Meta missing for {s_id}"
            if not IS_PROD:
                assert False, msg
            else:
                logger.warning(msg)

validate_meta()
# ------------------------------------------------

def get_db_cursor():
    return conn.cursor()



def calculate_objectives_v3(current_price, history_rows):
    """
    Python implementation of Trading Objective V3 Â§7, Â§8, Â§9
    history_rows: newest first (close, high, low)
    """
    if len(history_rows) < 120:
        return None

    # SMA Calculations (Newest first, but SMA needs oldest first or reversed indexing)
    closes = [r["close"] for r in history_rows]
    highs = [r["high"] for r in history_rows]
    lows = [r["low"] for r in history_rows]

    def sma(period):
        if len(closes) < period: return None
        return sum(closes[:period]) / period

    def atr(period=14):
        if len(history_rows) < period + 1: return None
        tr_sum = 0
        for i in range(period):
            h = highs[i]
            l = lows[i]
            pc = closes[i+1]
            tr = max(h - l, abs(h - pc), abs(l - pc))
            tr_sum += tr
        return tr_sum / period

    def rsi(period=14):
        if len(closes) < period + 1: return 50
        deltas = []
        for i in range(period):
            deltas.append(closes[i] - closes[i+1])
        gains = [d if d > 0 else 0 for d in deltas]
        losses = [abs(d) if d < 0 else 0 for d in deltas]
        avg_gain = sum(gains) / period
        avg_loss = sum(losses) / period
        if avg_loss == 0: return 100
        rs = avg_gain / avg_loss
        return 100 - (100 / (1 + rs))

    ma5, ma10, ma20, ma60, ma120 = sma(5), sma(10), sma(20), sma(60), sma(120)
    current_atr = atr(14)
    current_rsi = rsi(14)

    recent_high = max(highs[:20])
    recent_low_short = min(lows[:20])
    recent_low_mid = min(lows[:60])
    recent_low_long = min(lows[:120])

    def solve(timeframe):
        # 1. Scoring Logic (Â§7)
        trend_score = 0
        if ma20 and ma60 and ma120:
            if ma20 > ma60 > ma120: trend_score = 30
            elif ma20 > ma60: trend_score = 20
            else: trend_score = -30
        
        momentum_score = 0
        if 50 <= current_rsi <= 65: momentum_score = 10
        elif current_rsi > 70: momentum_score = -10
        elif current_rsi < 30: momentum_score = -5

        volatility_adj = 0
        vol_ratio = (current_atr / current_price) * 100 if current_atr else 0
        if vol_ratio < 3: volatility_adj = 5
        elif vol_ratio > 8: volatility_adj = -15

        base_score = 50 + trend_score + momentum_score + volatility_adj

        # 2. Entry / Stop / Target (Â§8)
        entry = current_price
        multiplier, rr, min_low = 2.0, 2.5, recent_low_mid
        
        if timeframe == 'short':
            entry = min(current_price, ma5 or current_price, ma10 or current_price)
            multiplier, rr, min_low = 1.5, 2.0, recent_low_short
        elif timeframe == 'mid':
            entry = min(current_price, ma20 or current_price)
            multiplier, rr, min_low = 2.0, 2.5, recent_low_mid
        else:
            entry = min(current_price, ma60 or current_price)
            multiplier, rr, min_low = 3.0, 3.0, recent_low_long

        stop = entry - (current_atr * multiplier) if current_atr else entry * 0.95
        stop = max(stop, min_low)

        risk_penalty = 0
        if entry > 0 and (entry - stop) / entry < 0.03: risk_penalty -= 10
        
        target = entry + (entry - stop) * rr
        target = min(target, recent_high if recent_high > entry else target)

        if (entry - stop) > 0 and (target - entry) / (entry - stop) < 2.0: risk_penalty -= 10

        final_score = max(0, min(100, base_score + risk_penalty))

        # 3. Status Mapping (Â§6)
        status = 'AVOID'
        if final_score >= 70: status = 'ACTIVE'
        elif final_score >= 40: status = 'WAIT'

        # 4. Flags (Â§5)
        flags = []
        if ma20 and ma60 and ma120 and ma20 > ma60 > ma120: flags.append('UPTREND_CONFIRMED')
        if ma20 and ma60 and ma20 < ma60: flags.append('BROKEN_TREND')
        if ma20 and abs(current_price - ma20) / ma20 < 0.01: flags.append('TREND_WEAK')
        if current_rsi > 70: flags.append('OVERBOUGHT')
        if current_rsi < 30: flags.append('OVERSOLD')
        if vol_ratio > 5: flags.append('HIGH_VOLATILITY')

        # 5. Strategy (Â§9)
        strategy = 'NO_TRADE'
        if status == 'AVOID' or 'BROKEN_TREND' in flags:
            strategy = 'NO_TRADE'
        elif 'UPTREND_CONFIRMED' in flags and current_rsi < 65:
            strategy = 'PULLBACK_TREND'
        elif 'HIGH_VOLATILITY' in flags and current_price >= recent_high:
            strategy = 'BREAKOUT'
        elif 'OVERSOLD' in flags and 'TREND_WEAK' in flags:
            strategy = 'MEAN_REVERSION'

        # 6. Stop/Target Rules (Â§8)
        if stop >= entry or target <= entry:
            status = 'WAIT'

        # Human Reason (Â§10)
        reason = ""
        if status == 'AVOID': reason = f"ì ìˆ˜ ë¯¸ë‹¬ ({final_score}ì ). í•˜ë½ ì¶”ì„¸ ë˜ëŠ” ê³¼ë„í•œ ë¦¬ìŠ¤í¬ë¡œ ì¸í•´ ì œì™¸ë©ë‹ˆë‹¤."
        elif status == 'WAIT': reason = f"ê´€ë§ êµ¬ê°„ ({final_score}ì ). ì§€ì§€ì„  í™•ì¸ ë˜ëŠ” ì¶”ì„¸ ê°•í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        else: reason = f"ì •ë°°ì—´ ë° ëª¨ë©˜í…€ ì–‘í˜¸ ({final_score}ì ). {strategy} ì „ëµ ê¸°ë°˜ ë§¤ìˆ˜ ê²€í† ."

        return {
            "status": status,
            "score": final_score,
            "strategy": strategy,
            "confidenceFlags": flags,
            "reason": reason,
            "entry": round(entry, -1) if status == 'ACTIVE' else None,
            "stop": round(stop, -1) if status == 'ACTIVE' else None,
            "target": round(target, -1) if status == 'ACTIVE' else None
        }

    return {
        "short": solve('short'),
        "mid": solve('mid'),
        "long": solve('long')
    }

def process_watchlist(tickers):
    """
    Analyze user's interested tickers and upload reports.
    Optimized: Bulk fetch and group in memory.
    """
    if not tickers:
        return

    normalized_tickers = list(set([t.split('.')[0] for t in tickers]))
    cur = get_db_cursor()
    
    placeholders = ','.join(['?'] * len(normalized_tickers))
    
    price_sql = f"""
        SELECT code, date, close, high, low, market_cap, per, pbr
        FROM daily_price 
        WHERE code IN ({placeholders})
        ORDER BY code, date DESC
    """
    
    supply_sql = f"""
        SELECT code, date, foreigner, institution 
        FROM daily_supply
        WHERE code IN ({placeholders})
        ORDER BY code, date DESC
    """
    
    try:
        cur.execute(price_sql, normalized_tickers)
        price_rows = cur.fetchall()
        
        cur.execute(supply_sql, normalized_tickers)
        supply_rows = cur.fetchall()
        
        price_map = {}
        for r in price_rows:
            if r["code"] not in price_map: price_map[r["code"]] = []
            price_map[r["code"]].append(dict(r))
            
        supply_map = {}
        for r in supply_rows:
            if r["code"] not in supply_map: supply_map[r["code"]] = []
            supply_map[r["code"]].append(dict(r))
            
        reports = []
        for code in normalized_tickers:
            p_history = price_map.get(code, [])
            s_history = supply_map.get(code, [])
            
            if not p_history: continue
            
            # 1. Technical Analysis V3
            latest_price = p_history[0]["close"]
            obj_v3 = calculate_objectives_v3(latest_price, p_history)
            
            # Check Â§11.2: Proceed even if AVOID to ensure fundamentals/supply are visible in UI
            if obj_v3:
                all_avoid = all(obj_v3[tf]["status"] == 'AVOID' for tf in ['short', 'mid', 'long'])
                if all_avoid:
                    logger.info(f"ğŸ“Š {code} is Status: AVOID (Uploading minimal report for UI visibility)")

            # 2. Supply Analysis
            c_map = {p["date"]: p["close"] for p in p_history[:200]}
            supply_chart = []
            
            f_net_5 = sum(s["foreigner"] for s in s_history[:5])
            i_net_5 = sum(s["institution"] for s in s_history[:5])
            f_net_20 = sum(s["foreigner"] for s in s_history[:20])
            i_net_20 = sum(s["institution"] for s in s_history[:20])
            
            for s in reversed(s_history[:200]):
                supply_chart.append({
                    "date": s["date"],
                    "foreigner": s["foreigner"],
                    "institution": s["institution"],
                    "close": c_map.get(s["date"])
                })
            
            # 3. Merge & Summary
            latest_p = p_history[0]
            # Use mid-term strategy for overall summary if ACTIVE, otherwise reason
            main_obj = obj_v3["mid"] if obj_v3 else None
            summary = main_obj["reason"] if main_obj else "ë°ì´í„° ë¶„ì„ ì¤‘..."

            report = {
                "date": TODAY,
                "ticker": code,
                "report_data": {
                    "v3_objectives": obj_v3,
                    "summary": summary,
                    "trend": main_obj["status"] if main_obj else "NEUTRAL",
                    "technical_score": main_obj["score"] if main_obj else 0,
                    "supply_chart": supply_chart,
                    "metrics": {
                        "foreigner_5d_net": f_net_5,
                        "institution_5d_net": i_net_5,
                        "foreigner_20d_net": f_net_20,
                        "institution_20d_net": i_net_20
                    },
                    "fundamentals": {
                        "market_cap": latest_p.get("market_cap"),
                        "per": latest_p.get("per"),
                        "pbr": latest_p.get("pbr")
                    }
                }
            }
            reports.append(report)

        if reports:
            supabase.table("daily_analysis_reports").upsert(reports, on_conflict="ticker").execute()
            logger.info(f"âœ… Uploaded {len(reports)} V3 reports to Supabase.")
            
    except Exception as e:
        logger.error(f"Watchlist processing failed: {e}")

def run_algo_screening():
    """
    Filter all stocks for specific strategies (Algo Picks).
    v5 Spec: Dynamic thresholds, multi-level sorting, and group-based confluence.
    """
    logger.info("ğŸ•µï¸ Running Algorithm Screening (Algo Picks v5)...")
    cur = get_db_cursor()
    
    # 0. Initial Setup & Universe Check
    cur.execute("SELECT MAX(date) FROM daily_price")
    max_price_date = cur.fetchone()[0]
    cur.execute("SELECT MAX(date) FROM daily_supply")
    max_supply_date = cur.fetchone()[0]

    if not max_price_date:
        logger.warning("No data found in DB. Skipping Algo Screening.")
        return []

    # Calculate Dynamic Mcap Threshold (Top 70% of Active Universe)
    cur.execute("""
        SELECT market_cap FROM daily_price p
        JOIN tickers t ON p.code = t.code
        WHERE p.date = ? AND t.is_active = 1 AND p.market_cap > 0
    """, (max_price_date,))
    mcap_universe = [r[0] for r in cur.fetchall()]
    u_size = len(mcap_universe)
    
    if u_size > 0:
        mcap_universe.sort()
        # Top 70% means the 30th percentile from the bottom
        idx = int(u_size * 0.3)
        mcap_threshold_dynamic = mcap_universe[idx]
    else:
        mcap_threshold_dynamic = 300000000000 # Fallback 300B
    
    GLOBAL_MCAP_MIN = max(300000000000, mcap_threshold_dynamic)
    logger.info(f"ğŸ“Š Mcap Universe: {u_size} stocks. Threshold: {GLOBAL_MCAP_MIN/1e8:.1f}B Won (Top 70% vs 300B)")

    def get_tech_status(ticker_code):
        """Fetch history and calculate V3 status for screening safety."""
        cur.execute("""
            SELECT close, high, low, date 
            FROM daily_price 
            WHERE code = ? 
            ORDER BY date DESC LIMIT 150
        """, (ticker_code,))
        rows = cur.fetchall()
        if len(rows) < 120: return "UNKNOWN"
        
        hist = [dict(r) for r in rows]
        res = calculate_objectives_v3(hist[0]["close"], hist)
        if not res: return "UNKNOWN"
        
        # If any timeframe is NOT AVOID, we consider it OK/WAIT
        # But for strict momentum, we'll check 'mid'
        status = res['mid']['status']
        return status

    strategies_raw = {} # {strategy_id: [candidates]}
    filter_counts = {s_id: {"Mcap": 0, "NetIncome": 0, "Technical": 0, "Other": 0} for s_id in STRATEGY_META}

    def get_mcap_limit(s_id):
        override = STRATEGY_META[s_id].get("mcap_override")
        return override if override is not None else GLOBAL_MCAP_MIN

    # Strategy 1: Value Picks
    # Priority: Profit Quality DESC -> PER ASC -> PBR ASC
    mcap_limit_val = get_mcap_limit("Value_Picks")
    cur.execute("""
        SELECT p.code, p.per, p.pbr, p.roe, p.operating_margin, p.market_cap, p.eps
        FROM daily_price p
        JOIN tickers t ON p.code = t.code
        WHERE p.date = ? AND t.is_active = 1
    """, (max_price_date,))
    
    val_candidates = []
    for r in cur.fetchall():
        d = dict(r)
        code = d['code']
        mcap = d['market_cap'] or 0
        # Filters
        if mcap < mcap_limit_val:
            if code in DEBUG_TICKERS: logger.debug(f"[Value_Picks][{code}] Drop: Mcap {mcap} < {mcap_limit_val}")
            filter_counts["Value_Picks"]["Mcap"] += 1
            continue
        if (d['eps'] or 0) <= 0: # NetIncome Check
            if code in DEBUG_TICKERS: logger.debug(f"[Value_Picks][{code}] Drop: NetIncome <= 0")
            filter_counts["Value_Picks"]["NetIncome"] += 1
            continue
        if not (0 < d['per'] < 30 and 0.3 <= d['pbr'] < 1.2 and (d['roe'] or 0) >= 8):
            if code in DEBUG_TICKERS: logger.debug(f"[Value_Picks][{code}] Drop: PER/PBR/ROE range fail")
            filter_counts["Value_Picks"]["Other"] += 1
            continue
        
        # Scoring
        profit_quality = (d['roe'] or 0) * 0.6 + (d['operating_margin'] or 0) * 0.4
        
        # Technical Status for Value Picks (Warning only, no exclusion)
        t_status = get_tech_status(code)
        
        val_candidates.append({
            "ticker": code,
            "sort_key": (-profit_quality, d['per'], d['pbr']),
            "metrics": {"profit_quality": profit_quality, "per": d['per'], "pbr": d['pbr']},
            "technical_status": t_status
        })
    val_candidates.sort(key=lambda x: x["sort_key"])
    strategies_raw["Value_Picks"] = val_candidates[:15]

    # Strategy 2: Twin Engines
    # Priority: Demand Power DESC -> Co-momentum DESC -> Total Buy DESC
    mcap_limit_twin = get_mcap_limit("Twin_Engines")
    cur.execute("""
        SELECT s.code, s.foreigner, s.institution, p.market_cap, p.close
        FROM daily_supply s
        JOIN daily_price p ON s.code = p.code AND s.date = p.date
        JOIN tickers t ON s.code = t.code
        WHERE s.date = ? AND t.is_active = 1
        AND s.foreigner > 0 AND s.institution > 0
    """, (max_supply_date,))
    
    twin_candidates = []
    for r in cur.fetchall():
        d = dict(r)
        code = d['code']
        mcap = d['market_cap'] or 1
        f_buy, i_buy = d['foreigner'], d['institution']
        
        if mcap < mcap_limit_twin:
            if code in DEBUG_TICKERS: logger.debug(f"[Twin_Engines][{code}] Drop: Mcap {mcap} < {mcap_limit_twin}")
            filter_counts["Twin_Engines"]["Mcap"] += 1
            continue
            
        demand_power = ((f_buy + i_buy) / mcap) * 100
        if demand_power < 0.05:
            if code in DEBUG_TICKERS: logger.debug(f"[Twin_Engines][{code}] Drop: Demand Power {demand_power:.3f}% < 0.05%")
            filter_counts["Twin_Engines"]["Other"] += 1
            continue
            
        co_momentum = min(f_buy, i_buy)
        
        # 2. Technical Safety Filter (Strict Exclusion for Momentum)
        t_status = get_tech_status(code)
        if t_status == "AVOID":
            if code in DEBUG_TICKERS: logger.debug(f"[Twin_Engines][{code}] Drop: Technical Status AVOID")
            filter_counts["Twin_Engines"]["Technical"] += 1
            continue

        twin_candidates.append({
            "ticker": code,
            "sort_key": (-demand_power, -co_momentum, -(f_buy + i_buy)),
            "metrics": {"demand_power": demand_power, "co_momentum": co_momentum},
            "technical_status": t_status
        })
    twin_candidates.sort(key=lambda x: x["sort_key"])
    strategies_raw["Twin_Engines"] = twin_candidates[:15]

    # Strategy 3: Foreigner Accumulation
    # Priority: Accumulation Density DESC -> 21d Acc DESC -> Box Range ASC
    mcap_limit_acc = get_mcap_limit("Foreigner_Accumulation")
    # Simplify query: Fetch 21d sum for all active tickers
    cur.execute("""
        SELECT code, SUM(foreigner) as f_sum 
        FROM daily_supply 
        WHERE date >= strftime('%Y%m%d', 'now', '-21 days')
        GROUP BY code HAVING f_sum > 0
    """)
    acc_stats = {r[0]: r[1] for r in cur.fetchall()}
    
    acc_candidates = []
    for code, f_sum in acc_stats.items():
        cur.execute("""
            SELECT MAX(close) as h, MIN(close) as l, market_cap 
            FROM daily_price WHERE code = ? AND date >= strftime('%Y%m%d', 'now', '-21 days')
        """, (code,))
        p = cur.fetchone()
        if not p: continue
        mcap = p['market_cap'] or 0
        if not mcap: continue
        
        if mcap < mcap_limit_acc:
            if code in DEBUG_TICKERS: logger.debug(f"[Foreigner_Acc][{code}] Drop: Mcap {mcap} < {mcap_limit_acc}")
            filter_counts["Foreigner_Accumulation"]["Mcap"] += 1
            continue
            
        box_range = (p['h'] - p['l']) / p['l'] if p['l'] > 0 else 1.0
        if box_range > 0.12:
            if code in DEBUG_TICKERS: logger.debug(f"[Foreigner_Acc][{code}] Drop: Box Range {box_range:.2%} > 12%")
            filter_counts["Foreigner_Accumulation"]["Other"] += 1
            continue
            
        density = (f_sum / p['market_cap']) * 100
        
        # 2. Technical Safety Filter (Strict Exclusion for Momentum)
        t_status = get_tech_status(code)
        if t_status == "AVOID":
            if code in DEBUG_TICKERS: logger.debug(f"[Foreigner_Acc][{code}] Drop: Technical Status AVOID")
            filter_counts["Foreigner_Accumulation"]["Technical"] += 1
            continue

        acc_candidates.append({
            "ticker": code,
            "sort_key": (-density, -f_sum, box_range),
            "metrics": {"acc_density": density, "acc_21d": f_sum, "box_range": box_range},
            "technical_status": t_status
        })
    acc_candidates.sort(key=lambda x: x["sort_key"])
    strategies_raw["Foreigner_Accumulation"] = acc_candidates[:15]

    # Strategy 4: Trend Following
    # Priority: Vol Power DESC -> Trend Score DESC -> Breakout Age ASC
    mcap_limit_trend = get_mcap_limit("Trend_Following")
    cur.execute("""
        SELECT p.code, p.close, p.open, p.high, p.volume, p.market_cap
        FROM daily_price p JOIN tickers t ON p.code = t.code
        WHERE p.date = ? AND t.is_active = 1 AND p.close > p.open
    """, (max_price_date,))
    
    trend_candidates = []
    for r in cur.fetchall():
        d = dict(r)
        code = d['code']
        mcap = d['market_cap'] or 0
        if mcap < mcap_limit_trend:
            if code in DEBUG_TICKERS: logger.debug(f"[Trend_Following][{code}] Drop: Mcap {mcap} < {mcap_limit_trend}")
            filter_counts["Trend_Following"]["Mcap"] += 1
            continue
            
        # Wick Check
        body = d['close'] - d['open']
        upper_wick = d['high'] - d['close']
        if upper_wick >= body:
            if code in DEBUG_TICKERS: logger.debug(f"[Trend_Following][{code}] Drop: Wick {upper_wick} >= Body {body}")
            filter_counts["Trend_Following"]["Other"] += 1
            continue
            
        # Vol Power
        cur.execute("SELECT volume FROM daily_price WHERE code = ? AND date < ? ORDER BY date DESC LIMIT 20", (code, max_price_date))
        v_hist = [h[0] for h in cur.fetchall()]
        if len(v_hist) < 20: 
            if code in DEBUG_TICKERS: logger.debug(f"[Trend_Following][{code}] Drop: Insufficient volume history")
            continue
        avg_v20 = sum(v_hist) / 20
        vol_power = min(5.0, d['volume'] / avg_v20) if avg_v20 > 0 else 0
        if vol_power < 1.5: 
            if code in DEBUG_TICKERS: logger.debug(f"[Trend_Following][{code}] Drop: Vol Power {vol_power:.2f} < 1.5")
            continue
            
        # 2. Technical Safety Filter (Strict Exclusion for Momentum)
        t_status = get_tech_status(code)
        if t_status == "AVOID":
            if code in DEBUG_TICKERS: logger.debug(f"[Trend_Following][{code}] Drop: Technical Status AVOID")
            filter_counts["Trend_Following"]["Technical"] += 1
            continue
        
        # Trend Score & Breakout Age
        cur.execute("SELECT close, high FROM daily_price WHERE code = ? ORDER BY date DESC LIMIT 120", (code,))
        h_data = cur.fetchall()
        h_closes = [h[0] for h in h_data]
        h_highs = [h[1] for h in h_data]
        
        ma20, ma60, ma120 = sum(h_closes[:20])/20, sum(h_closes[:60])/60, sum(h_closes[:120])/120
        trend_score = 30 if ma20 > ma60 > ma120 else (20 if ma20 > ma60 else 0)
        
        # Breakout Age (Simplified: days since 20d high)
        max_h20 = max(h_highs[1:21])
        breakout_age = 0 if d['high'] > max_h20 else 99 # Simplified for now
        
        trend_candidates.append({
            "ticker": code,
            "sort_key": (-vol_power, -trend_score, breakout_age),
            "metrics": {"vol_power": vol_power, "trend_score": trend_score},
            "technical_status": t_status
        })
    trend_candidates.sort(key=lambda x: x["sort_key"])
    strategies_raw["Trend_Following"] = trend_candidates[:15]

    # --- ğŸ•µï¸ Confluence & Final Ranking ---
    # Log Filter Summaries
    for s_id, counts in filter_counts.items():
        logger.info(f"  [{s_id}] Filtered: Mcap={counts['Mcap']}, NetIncome={counts['NetIncome']}, Technical={counts['Technical']}, Other={counts['Other']}")

    all_found_tickers = {} # {ticker: {rank_sum, count, groups, best_rank}}
    for s_id, candidates in strategies_raw.items():
        if not candidates:
            logger.info(f"  [{s_id}] NO_QUALIFIED_CANDIDATES")
            continue
            
        for rank, cand in enumerate(candidates, 1):
            ticker = cand["ticker"]
            if ticker not in all_found_tickers:
                all_found_tickers[ticker] = {"rank_sum": 0, "count": 0, "groups": set(), "best_rank": 999}
            
            stats = all_found_tickers[ticker]
            stats["rank_sum"] += rank
            stats["count"] += 1
            stats["groups"].add(STRATEGY_META[s_id]["group"])
            stats["best_rank"] = min(stats["best_rank"], rank)

    # Calculate Weighted Confluence
    confluence_list = []
    for ticker, stats in all_found_tickers.items():
        weighted_group_score = sum(GROUP_WEIGHT.get(g, 1.0) for g in stats["groups"])
        avg_rank = stats["rank_sum"] / stats["count"]
        confluence_list.append({
            "ticker": ticker,
            "weighted_group_score": weighted_group_score,
            "best_rank": stats["best_rank"],
            "avg_rank": avg_rank,
            "groups": list(stats["groups"]),
            "technical_status": get_tech_status(ticker) # Fetch status for confluence items
        })

    # Sort Confluence: Group Score DESC -> Best Rank ASC -> Avg Rank ASC
    confluence_list.sort(key=lambda x: (-x["weighted_group_score"], x["best_rank"], x["avg_rank"]))
    final_confluence = confluence_list[:5]

    # --- ğŸ“¦ Payload Construction & Sync ---
    picks_payload = []
    # 1. Strategy Picks (Top 5 each)
    for s_id, candidates in strategies_raw.items():
        status = "OK" if candidates else "NO_QUALIFIED_CANDIDATES"
        tickers = [c["ticker"] for c in candidates[:5]]
        
        details = {
            "status": status,
            "meta_version": "v5.0",
            "analyzer_ver": "1.0",
            "snapshots": {
                "mcap_threshold": GLOBAL_MCAP_MIN,
                "universe_size": u_size,
                "group_weights": GROUP_WEIGHT
            },
            "candidates": {
                c["ticker"]: {
                    "rank": i+1,
                    "metrics": c["metrics"],
                    "technical_status": c.get("technical_status", "UNKNOWN")
                } for i, c in enumerate(candidates[:5])
            }
        }
        picks_payload.append({
            "date": TODAY,
            "strategy_name": s_id,
            "tickers": tickers,
            "details": details
        })

    # 2. Confluence Pick (Unified Strategy)
    confluence_tickers = [c["ticker"] for c in final_confluence]
    picks_payload.append({
        "date": TODAY,
        "strategy_name": "Confluence_Top",
        "tickers": confluence_tickers,
        "details": {
            "status": "OK" if confluence_tickers else "NONE",
            "meta_version": "v5.0",
            "items": [
                {
                    "ticker": c["ticker"],
                    "weighted_group_score": c["weighted_group_score"],
                    "best_rank": c["best_rank"],
                    "avg_rank": c["avg_rank"],
                    "groups": c["groups"],
                    "technical_status": c.get("technical_status", "UNKNOWN")
                } for c in final_confluence
            ]
        }
    })

    try:
        supabase.table("algo_picks").upsert(picks_payload, on_conflict="strategy_name").execute()
        logger.info(f"âœ… Uploaded {len(picks_payload)} Algo Picks to Supabase (v5)")
        
        # Send Notification
        notify_telegram(picks_payload)
    except Exception as e:
        logger.error(f"Algo Upload Error: {e}")
    
    return list(set([t for p in picks_payload for t in p["tickers"]]))

def get_telegram_settings_from_db():
    """
    Fetch Telegram credentials from Supabase user_settings table.
    """
    try:
        response = supabase.table("user_settings").select("telegram_chat_id, telegram_bot_token").limit(1).execute()
        if response.data:
            settings = response.data[0]
            return settings.get("telegram_chat_id"), settings.get("telegram_bot_token")
    except Exception as e:
        logger.warning(f"Failed to fetch Telegram settings: {e}")
    return None, None

def notify_telegram(picks_payload):
    """
    Send a summary message to Telegram based on v5 picks payload.
    """
    db_chat_id, db_token = get_telegram_settings_from_db()
    token = db_token or os.getenv("TELEGRAM_BOT_TOKEN")
    chat_id = db_chat_id or os.getenv("TELEGRAM_CHAT_ID")
    
    if not token or not chat_id:
        logger.warning("Telegram credentials missing. Skipping notification.")
        return

    # Extract counts and top picks
    summary_map = {p["strategy_name"]: len(p["tickers"]) for p in picks_payload}
    confluence_payload = next((p for p in picks_payload if p["strategy_name"] == "Confluence_Top"), None)
    confluence_tickers = confluence_payload["tickers"] if confluence_payload else []
    
    # Link to production URL if possible, otherwise localhost
    site_url = os.getenv("NEXT_PUBLIC_SITE_URL", "https://daily-port.vercel.app")
    
    ticker_list_str = ', '.join(confluence_tickers[:5]) if confluence_tickers else 'None'
    message = f"""
ğŸš€ *DailyPort ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ ì™„ë£Œ* ({TODAY})

ğŸ” *ì „ëµë³„ ë§¤ì¹­ í˜„í™©*:
â€¢ ğŸ’ ì €í‰ê°€(Value): {summary_map.get('Value_Picks', 0)}ê°œ
â€¢ ğŸ¯ ë™ë°˜ë§¤ìˆ˜(Twin): {summary_map.get('Twin_Engines', 0)}ê°œ
â€¢ ğŸ¥ª ì™¸ì¸ë§¤ì§‘(Acc): {summary_map.get('Foreigner_Accumulation', 0)}ê°œ
â€¢ ğŸ“ˆ ì¶”ì„¸ì¶”ì¢…(Trend): {summary_map.get('Trend_Following', 0)}ê°œ

ğŸ† *í†µí•© ì¶”ì²œ (Confluence)*:
`{ticker_list_str}`

[ì•Œê³ ë¦¬ì¦˜ í”½ í™•ì¸í•˜ê¸°]({site_url}/algo-picks)
"""
    try:
        url = f"https://api.telegram.org/bot{token}/sendMessage"
        payload = {"chat_id": chat_id, "text": message, "parse_mode": "Markdown"}
        requests.post(url, json=payload, timeout=5)
        logger.info(f"ğŸ“¨ Telegram notification sent.")
    except Exception as e:
        logger.error(f"Telegram Error: {e}")

if __name__ == "__main__":
    logger.info("ğŸš€ Starting Daily Analyzer...")
    
    # 1. Fetch Tickers from Supabase (Watchlist + Portfolio)
    all_tickers = []
    try:
        # Fetch tickers from watchlists and portfolios
        watchlist_res = supabase.table("watchlists").select("ticker").execute()
        portfolio_res = supabase.table("portfolios").select("ticker").execute()
        
        watchlist_tickers = [r["ticker"] for r in watchlist_res.data] if watchlist_res.data else []
        portfolio_tickers = [r["ticker"] for r in portfolio_res.data] if portfolio_res.data else []
        
        # Merge and deduplicate
        all_tickers = list(set(watchlist_tickers + portfolio_tickers))
        
        logger.info(f"Found {len(watchlist_tickers)} watchlist tickers, {len(portfolio_tickers)} portfolio tickers")
        logger.info(f"Total unique tickers to analyze: {len(all_tickers)}")
            
    except Exception as e:
        logger.warning(f"Failed to fetch tickers from DB: {e}")
        
    # Fallback to defaults if empty
    if not all_tickers:
        all_tickers = ["005930", "000660", "035420", "035720"]
        logger.info("Using default tickers as no tickers found in Supabase.")
    
    # 2. Add Algo Picks to the queue
    algo_tickers = run_algo_screening()
    
    # Merge and deduplicate
    all_tickers = list(set(all_tickers + algo_tickers))
    logger.info(f"Final analysis queue: {len(all_tickers)} unique tickers")
    
    # 3. Process all
    process_watchlist(all_tickers)
    
    conn.close()
    logger.info("ğŸ‰ Analyzer Finished.")
